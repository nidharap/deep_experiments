{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Keras (Predict next work given x previous words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### keras version 2.1.2\n",
    "##### tensorflow version 1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN:\n",
    "\n",
    "1. We have a finite length of input sequence and hence we unroll the network to match that \n",
    "\n",
    "<img src=\"img/rnn_unrolled.png\">\n",
    "\n",
    "2. Each word in the above image is embedded into some space (typically w2v embeddings of some sort)\n",
    "\n",
    "3. Configurations for RNNs could be many-to-many , many-to-one, one-to-many\n",
    "e.g. : \n",
    "\n",
    "<img src=\"img/rnn_configs.png\">\n",
    "\n",
    "4. Weights are shared across steps\n",
    "\n",
    "5. RNNs have an issue of vanishing gradient problem -> multiplying multile gradient backwards in time, with each being a really small number or very large -> hence resulting a valishing or exploding gradient problem\n",
    "for small gradients -> results is an almost zero gradient flowing back all the way and hence the input at time step t-10 will have almost no effect at time t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math behind LSTM: \n",
    "https://www.youtube.com/watch?v=9zhrxE5PQgY\n",
    "\n",
    "Attention is all you need -\n",
    "\n",
    "- https://www.youtube.com/watch?v=iDulhoQ2pro&t=1305s\n",
    "- https://www.youtube.com/watch?v=rBCqOTEfxvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we have -\n",
    "- each word embedding in 650 dimensions\n",
    "- with a seq size of 35 \n",
    "- a batch size of 20 \n",
    "\n",
    "Hence, the tensor input will be 20 X 35 X 650 , reordered into 35 X 20 X 650 to make it \"time major\" format\n",
    "\n",
    "The (20, 35, 650) is then flattened and fed into softmax for a classification problem i.e. \n",
    "(20, 35, 650) => (700, 650) and then fed into softmax\n",
    "\n",
    "\n",
    "<img src=\"img/target_state_to_achieve.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Predict next word, given previous x words \n",
    "\n",
    "Sentence: The cat ate the mouse\n",
    "X -> The cat ate the\n",
    "y -> cat ate the mouse\n",
    "\n",
    "predict \"mouse\", given \"the cat ate the\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, Dropout, TimeDistributed, Reshape, Lambda\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read text file and convert to words with a <eos> for end of statement\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"rb\") as f:\n",
    "        return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "#build vocab so that each word is converted to an integer -> word to word_id\n",
    "def build_vocab(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "#convert files to seq of word_ids instead of seq of words\n",
    "def file_to_word_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download data from here - http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=\"\", verbose=True):\n",
    "    # get the data paths\n",
    "    train_path = os.path.join(data_path, \"ptb.train.txt\")\n",
    "    valid_path = os.path.join(data_path, \"ptb.valid.txt\")\n",
    "    test_path = os.path.join(data_path, \"ptb.test.txt\")\n",
    "\n",
    "    # build the complete vocabulary, then convert text data to list of integers\n",
    "    word_to_id = build_vocab(train_path)\n",
    "    train_data = file_to_word_ids(train_path, word_to_id)\n",
    "    valid_data = file_to_word_ids(valid_path, word_to_id)\n",
    "    test_data = file_to_word_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "    \n",
    "    if verbose:\n",
    "        print(train_data[:5])\n",
    "        print(word_to_id)\n",
    "        print(vocabulary)\n",
    "        print(\" \".join([reversed_dictionary[x] for x in train_data[:10]]))\n",
    "        \n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/nikhildharap/ml_nlp_experiments/deep_experiments/LSTM_with_TF_Keras/simple-examples/data\"\n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data(data_path=data_path, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch generator , generates x and y that could be then fed to keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### skip steps is used to move the pointer of current position by some words \n",
    "e.g. \n",
    "sentence: \"the cat ate the mouse and ran away into the alley\"\n",
    "iter_1\n",
    "x => \"the cat ate the\"\n",
    "y => \"cat ate the mouse\"\n",
    "\n",
    "if skip_step == 4, \n",
    "iter_2 will look like \n",
    "x => \"mouse and ran away\"\n",
    "y => \"and ran away into\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        \n",
    "    def generate_batch():\n",
    "        #x will be batch_size X  num_steps\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        \n",
    "        #y will be batch_size X num_steps X vocab_size(one-hot encoded version of numbers)\n",
    "        # this converts seq of numbers to seq of one-hot encoded rep eventually (e.g. (100,1) => (100,10000))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        \n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    #reset the pointer to start of the dataset\n",
    "                    self.current_idx = 0\n",
    "                    \n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx+self.num_steps]\n",
    "                y_temp = self.data[self.current_idx+1:self.current_idx+self.num_steps+1]\n",
    "                \n",
    "                #convert all of the y_temp to one hot encoded representation\n",
    "                y[i, :] = to_categorical(y_temp, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 30\n",
    "batch_size = 20\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 500\n",
    "use_dropout=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Embedding() layer takes -\n",
    "- the size of the vocabulary as its first argument\n",
    "- then the size of the resultant embedding vector that you want as the next argument\n",
    "- Finally, because this layer is the first layer in the network, we must specify the “length” of the input i.e. the number of steps/words in each sample.\n",
    "\n",
    "LSTM layer takes -\n",
    "- input size as the first argument, which is the hidden size for us . This is the input size for each LSTM cell i.e. __number of nodes in the hidden layers within the LSTM cell, e.g. the number of cells in the forget gate layer, the tanh squashing input layer and so on.__\n",
    "\n",
    "\n",
    "\n",
    "- return seq will return output from each LSTM cell\n",
    "\n",
    "\n",
    "<img src=\"img/LSTM_Cell_Return_Seq.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#the input to the embedding layer is (batch_size, num_steps) and the output is (batch_size, num_steps, hidden_size)\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "#add LSTM cells - two layers\n",
    "#output shape of LSTM layer is (batch_size, num_steps, hidden_size) because we want return_seq = True\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "#check for drop out\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    \n",
    "#there is a special Keras layer for use in recurrent neural networks called TimeDistributed. \n",
    "#This function adds an independent layer for each time step in the recurrent model. \n",
    "#So, for instance, if we have 10 time steps in a model, \n",
    "#a TimeDistributed layer operating on a Dense layer would produce 10 independent Dense layers, \n",
    "#one for each time step.\n",
    "\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after 40 epoch\n",
    "model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "dummy_iters = 40\n",
    "example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "                                                     skip_step=1)\n",
    "print(\"Training data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_training_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "print(true_print_out)\n",
    "print(pred_print_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://adventuresinmachinelearning.com/keras-lstm-tutorial/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
